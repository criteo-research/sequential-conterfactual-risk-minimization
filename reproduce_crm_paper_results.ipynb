{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5920198a-d1b6-4438-8bd2-7bddcc508a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3795c489-55be-449f-84d1-b1d1315cae89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66944fc2-db7d-424d-8670-7e929f5af40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53ffaff7-26f2-4259-a078-891e513523c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_utils import load_dataset\n",
    "from baselines_skylines import result_table, make_baselines_skylines\n",
    "from crm_dataset import CRMDataset\n",
    "from crm_model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d56b880-2fcc-46f2-8909-e75dba727ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************** scene ******************************\n",
      "X_train: (1805, 295) y_train: (1805, 6)\n",
      "learning baselines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 1805 train instances have 0 propensity\n",
      "split 0 elapsed: 0 s split 1 elapsed: 1 s split 2 elapsed: 3 s split 3 elapsed: 4 s split 4 elapsed: 6 s split 5 elapsed: 7 s split 6 elapsed: 9 s split 7 elapsed: 10 s split 8 elapsed: 12 s split 9 elapsed: 14 s \n",
      "CI(L(IPS) - L(POEM)) @ alpha=5%: [-0.0315 ; -0.0049] \n",
      "\n",
      "********************************************************************************\n",
      "Baseline -- crm paper: 0.257 -- vrcrm paper: 0.315 -- ours: 0.352\n",
      "IPS      -- crm paper: 0.199 -- vrcrm paper: 0.225 -- ours: 0.205\n",
      "POEM     -- crm paper: 0.195 -- vrcrm paper: 0.195 -- ours: 0.223\n",
      "Skyline  -- crm paper: 0.110 -- vrcrm paper: 0.237 -- ours: 0.144\n",
      "********************************************************************************\n",
      "\n",
      "****************************** yeast ******************************\n",
      "X_train: (1812, 104) y_train: (1812, 14)\n",
      "learning baselines\n",
      "0 / 1812 train instances have 0 propensity\n",
      "split 0 elapsed: 0 s split 1 elapsed: 1 s split 2 elapsed: 2 s split 3 elapsed: 4 s split 4 elapsed: 5 s split 5 elapsed: 7 s split 6 elapsed: 8 s split 7 elapsed: 10 s split 8 elapsed: 11 s split 9 elapsed: 13 s \n",
      "CI(L(IPS) - L(POEM)) @ alpha=5%: [-0.0059 ; 0.0504] \n",
      "\n",
      "********************************************************************************\n",
      "Baseline -- crm paper: 0.396 -- vrcrm paper: 0.392 -- ours: 0.473\n",
      "IPS      -- crm paper: 0.331 -- vrcrm paper: 0.304 -- ours: 0.341\n",
      "POEM     -- crm paper: 0.320 -- vrcrm paper: 0.320 -- ours: 0.318\n",
      "Skyline  -- crm paper: 0.163 -- vrcrm paper: 0.289 -- ours: 0.295\n",
      "********************************************************************************\n",
      "\n",
      "****************************** tmc2007 ******************************\n",
      "reducing dimension for TMC dataset\n",
      "X_train: (21447, 1000) y_train: (21447, 22)\n",
      "learning baselines\n",
      "0 / 21447 train instances have 0 propensity\n",
      "split 0 elapsed: 0 s split 1 elapsed: 28 s split 2 elapsed: 62 s split 3 elapsed: 87 s split 4 elapsed: 109 s split 5 elapsed: 133 s split 6 elapsed: 157 s split 7 elapsed: 186 s split 8 elapsed: 215 s split 9 elapsed: 237 s \n",
      "CI(L(IPS) - L(POEM)) @ alpha=5%: [0.0080 ; 0.1423] \n",
      "\n",
      "********************************************************************************\n",
      "Baseline -- crm paper: 0.152 -- vrcrm paper: 0.230 -- ours: 0.387\n",
      "IPS      -- crm paper: 0.128 -- vrcrm paper: 0.201 -- ours: 0.212\n",
      "POEM     -- crm paper: 0.100 -- vrcrm paper: 0.205 -- ours: 0.137\n",
      "Skyline  -- crm paper: 0.054 -- vrcrm paper: 0.056 -- ours: 0.089\n",
      "********************************************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dataset_name in ('scene', 'yeast', 'tmc2007',):\n",
    "    \n",
    "    print('*'*30, dataset_name, '*'*30)\n",
    "    \n",
    "    X_train, y_train, X_test, y_test, labels = load_dataset(dataset_name)\n",
    "    \n",
    "    print('learning baselines')\n",
    "    pi0, pistar = make_baselines_skylines(dataset_name, X_train, y_train)\n",
    "    \n",
    "    sampling_probas = pi0.predict_proba(X_train)\n",
    "    sampling_probas = np.array([_[:,1] for _ in sampling_probas]).T\n",
    "    \n",
    "    log_props_per_instance = jnp.log(sampling_probas).sum(axis=1)\n",
    "    n_zero_props = (jnp.exp(log_props_per_instance) == 0).astype(int).sum()\n",
    "    print(n_zero_props, '/', sampling_probas.shape[0], 'train instances have 0 propensity')\n",
    "    assert (n_zero_props / sampling_probas.shape[0]) < .1\n",
    "    \n",
    "    poem_lambda = {'scene':.5, 'yeast':.5, 'tmc2007':.5}[dataset_name]\n",
    "    \n",
    "    ips_losses = []\n",
    "    poem_losses = []\n",
    "    \n",
    "    start = time.time()\n",
    "    for i in range(10):\n",
    "        elapsed = int(time.time() - start)\n",
    "        print('split', i, 'elapsed:', elapsed, 's', end=' ')\n",
    "\n",
    "        # building CRM dataset\n",
    "        np.random.seed(i*42) # controlling action randomness   \n",
    "        crm_dataset = CRMDataset()\n",
    "        crm_dataset.update_from_supervised_dataset(\n",
    "            X_train, y_train, sampling_probas, \n",
    "            n_samples=4\n",
    "        )        \n",
    "        # print(\"Dataset ready:\", len(crm_dataset), 'hash:', crm_dataset.actions.sum(), file=sys.stderr)\n",
    "\n",
    "        # IPS\n",
    "        ips_model = Model.null_model(X_test.shape[1], y_test.shape[1])\n",
    "        ips_model.fit(\n",
    "            crm_dataset, \n",
    "            tol=1e-6,\n",
    "            maxiter=1000,\n",
    "            verbose = 0, \n",
    "            lambda_ = 0,\n",
    "            clip = 5e4,\n",
    "            snips = 1,\n",
    "        )\n",
    "        # print(\"%2d) IPS     E[HL]: %.3f | CRM Loss (++data): %.3f | CRM Loss: %.3f\" % (\n",
    "        #     i,\n",
    "        #     ips_model.expected_hamming_loss(X_test, y_test), \n",
    "        #     ips_model.crm_loss(ref_crm_dataset), \n",
    "        #     ips_model.crm_loss(crm_dataset)\n",
    "        # ))\n",
    "        ips_losses += [ips_model.expected_hamming_loss(X_test, y_test)]    \n",
    "\n",
    "        # POEM   \n",
    "        poem_model = Model.null_model(X_test.shape[1], y_test.shape[1])\n",
    "        poem_model.fit(\n",
    "            crm_dataset, \n",
    "            tol=1e-6,\n",
    "            maxiter=1000,\n",
    "            verbose=0, \n",
    "            clip = 5e4,\n",
    "            lambda_ = poem_lambda, \n",
    "            snips = 1,\n",
    "        )\n",
    "        # print(\"%2d) POEM    E[HL]: %.3f | CRM Loss (++data): %.3f | CRM Loss: %.3f\" % (\n",
    "        #     i,\n",
    "        #     poem_model.expected_hamming_loss(X_test, y_test), \n",
    "        #     poem_model.crm_loss(ref_crm_dataset), \n",
    "        #     poem_model.crm_loss(crm_dataset)\n",
    "        # ))\n",
    "        poem_losses += [poem_model.expected_hamming_loss(X_test, y_test)]    \n",
    "    print()\n",
    "\n",
    "    ips_losses = np.array(ips_losses)\n",
    "    poem_losses = np.array(poem_losses)\n",
    "\n",
    "    diff = ips_losses - poem_losses\n",
    "\n",
    "    print('CI(L(IPS) - L(POEM)) @ alpha=5%%: [%.4f ; %.4f]' % (\n",
    "        diff.mean() - 1.64*np.std(diff)/np.sqrt(10), diff.mean() + 1.64*np.std(diff)/np.sqrt(10)\n",
    "    ))\n",
    "    result_table(dataset_name, pi0, pistar, X_test, y_test, ips_loss=np.mean(ips_losses), poem_loss=np.mean(poem_losses))\n",
    "    print('*'*80)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
