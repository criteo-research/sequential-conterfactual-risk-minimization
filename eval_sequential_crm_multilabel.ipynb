{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd590c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1754dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import jax.numpy as np\n",
    "#import jaxopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86b7abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from sklearn.multioutput import MultiOutputClassifier, ClassifierChain\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da5c3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d5146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit\n",
    "from scipy.optimize import minimize as sp_minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c93204",
   "metadata": {},
   "source": [
    "---\n",
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaa9cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name='yeast'\n",
    "#dataset_name='scene'\n",
    "#dataset_name='tmc2007'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4757a860",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train_ = load_svmlight_file(dataset_name+'_train.svm', multilabel=True)\n",
    "X_train = np.array(X_train.todense())\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c48b812",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test_ = load_svmlight_file(dataset_name+'_test.svm', multilabel=True)\n",
    "X_test = np.array(X_test.todense())\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b3192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_labeller = MultiLabelBinarizer()\n",
    "y_train = onehot_labeller.fit_transform(y_train_).astype(int)\n",
    "y_test = onehot_labeller.transform(y_test_).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fdd9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = onehot_labeller.classes_.astype(int)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7514c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_train.sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82cf30b",
   "metadata": {},
   "source": [
    "FYI: Error rate of null policy (always predict 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105dcb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.sum()/(y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd6d4fd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9ef88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_hammingloss(p,y):\n",
    "    assert p.shape == y.shape\n",
    "    pos = np.where( (p != y) & (y > 0) )\n",
    "    neg = np.where( (p != y) & (y == 0) )\n",
    "    fn = p[neg].sum()\n",
    "    fp = (1-p[pos]).sum()\n",
    "    return (fn+fp)/(p.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04304ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_hammingloss(test_probas, y_test):\n",
    "    return np.mean([\n",
    "        micro_hammingloss(test_probas[:,k].reshape((len(y_test),1)), \n",
    "                          y_test[:,k].reshape((len(y_test),1))) \n",
    "        for k in range(y_test.shape[1])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3ab902",
   "metadata": {},
   "source": [
    "---\n",
    "## Our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1279d1",
   "metadata": {},
   "source": [
    "### CRM routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6917e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_crm_dataset(X, y, probas, n_samples=4, labels=labels):\n",
    "    \n",
    "    assert len(X) == len(y) == len(probas), (len(X) , len(y) , len(probas))\n",
    "    \n",
    "    P = []\n",
    "    A = []\n",
    "    F = []\n",
    "    R = []\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        \n",
    "        for k in range(n_samples):\n",
    "            \n",
    "            chosen_actions = [np.random.binomial(1, p=probas[i,j]) for j in labels]\n",
    "            chosen_actions_idx = [j for j in labels if chosen_actions[j] > 0]\n",
    "            chosen_actions_probas = [probas[i,j] for j in labels]\n",
    "            \n",
    "#             print(probas[i,:])\n",
    "#             print(chosen_actions_idx)\n",
    "#             print(chosen_actions_probas)\n",
    "            \n",
    "            A += [np.array(chosen_actions)]\n",
    "            P += [np.array(chosen_actions_probas)]\n",
    "            \n",
    "            x = X[i,:]\n",
    "            F += [x]\n",
    "\n",
    "            R += [np.array([ y[i,j]*chosen_actions[j] for j in labels ])]  # how do we scale that with chosen nber of actions ?\n",
    "#             print(R[-1])\n",
    "            \n",
    "    assert len(P) == len(X) * n_samples\n",
    "\n",
    "    return P, A, R, F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d83b97",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c8437a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(beta, features):\n",
    "    # beta is (k, d)\n",
    "    # features is (n, d)\n",
    "    p = np.dot(features, beta)\n",
    "    p = expit(p)\n",
    "    # predictions is (n,k)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239e2c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_model(beta, X, y, sampling_probas, prior_crm_dataset, samples_per_instance=4):\n",
    "    \n",
    "    # beta should be (k,d)\n",
    "    \n",
    "    assert beta.shape == (X.shape[1], y.shape[1])\n",
    "    \n",
    "    P, A, R, F = prior_crm_dataset\n",
    "       \n",
    "    newP, newA, newR, newF = generate_crm_dataset(\n",
    "        X, y, sampling_probas, n_samples=samples_per_instance\n",
    "    )\n",
    "    assert len(newP) == len(X)*samples_per_instance, (len(newP), len(X), samples_per_instance)\n",
    "    \n",
    "    # in form of list for easier stacking through different calls to iterate_model()\n",
    "    PP = P+newP \n",
    "    AA = A+newA\n",
    "    RR = R+newR\n",
    "    FF = F+newF\n",
    "    \n",
    "    # in form of arrays for model_predict()\n",
    "    P = np.array(PP) # (n,k) x [0,1] x R\n",
    "    A = np.array(AA) # (n,k) x {0,1}\n",
    "    R = np.vstack(RR) # (n,k) x {0,1}\n",
    "    F = np.vstack(FF) # (n,d) x [0,1] x R\n",
    "    \n",
    "    assert len(P) == len(A) == len(R) == len(F), (len(P), len(A), len(R), len(F))\n",
    "    assert P.shape[1] == A.shape[1] == R.shape[1]\n",
    "     \n",
    "    def crm_loss(beta):\n",
    "        beta = beta.reshape((X.shape[1], y.shape[1]))  # sp.minimize flattens the beta matrix\n",
    "        pred = model_predict(beta, F)\n",
    "        W = pred / P\n",
    "        l = np.sum((1-R)*W) / np.sum(W)\n",
    "        return l\n",
    "    \n",
    "    print('CRM iteration on %d samples - loss: %.4f -> ' % (len(PP), crm_loss(beta)), end='', file=sys.stderr)\n",
    "    \n",
    "    solution = sp_minimize(crm_loss, beta, method='L-BFGS-B')\n",
    "    newbeta = solution.x\n",
    "    \n",
    "    final_loss = crm_loss(newbeta)\n",
    "    \n",
    "    print('%.4f' % final_loss, file=sys.stderr)\n",
    "    \n",
    "    return newbeta.reshape((X.shape[1], y.shape[1])), (PP, AA, RR, FF), final_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d21fa2",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec13dda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(beta, X_test, y_test, normalize=False, binarize=True):\n",
    "    beta_test_probas = model_predict(beta, X_test)\n",
    "    if normalize:\n",
    "        beta_test_probas /= beta_test_probas.sum(axis=1).reshape((len(y_test),1))\n",
    "    if binarize:\n",
    "        beta_test_probas = (beta_test_probas > .5).astype(int)\n",
    "    return micro_hammingloss(beta_test_probas, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aef896",
   "metadata": {},
   "source": [
    "----\n",
    "## Baselines & Skylines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2170d08d",
   "metadata": {},
   "source": [
    " ![Perf from CRM article](./basesky.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e6493c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == 'tmc_2007':\n",
    "    print(\"reducing dimension for TMC dataset\")\n",
    "    fh = GaussianRandomProjection(n_components=1000)\n",
    "    X_train = fh.fit_transform(X_train)\n",
    "    X_test = fh.transform(X_test)\n",
    "    print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2550da71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"pi_null micro test loss:\", micro_hammingloss(np.zeros(y_test.shape), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1aab7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_dummy = MultiOutputClassifier(DummyClassifier())\n",
    "pi_dummy.fit(X_train, y_train)\n",
    "\n",
    "print(\"pi_dummy train loss:\", micro_hammingloss(pi_dummy.predict(X_train), y_train))\n",
    "print(\"pi_dummy test loss:\", micro_hammingloss(pi_dummy.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ae4729",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi0 = MultiOutputClassifier(LogisticRegression(), n_jobs=6)\n",
    "\n",
    "X_0, X_, y_0, y_ = train_test_split(X_train, y_train, test_size=.95, random_state=42)\n",
    "print('learning pi0 on', len(X_0), 'data points')\n",
    "pi0.fit(X_0, y_0)\n",
    "\n",
    "print(\"pi0 train loss:\", micro_hammingloss(pi0.predict(X_train), y_train))\n",
    "l0 = micro_hammingloss(pi0.predict(X_test), y_test)\n",
    "print(\"pi0 test loss:\", l0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a594a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pi0.estimators_)):\n",
    "    pi0.estimators_[i].coef_ += .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d59e93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "l0 = micro_hammingloss(pi0.predict(X_test), y_test)\n",
    "l0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc726f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "pistar = MultiOutputClassifier(LogisticRegressionCV(max_iter=10000, n_jobs=6))\n",
    "pistar.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b18ced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"pi* train loss:\", micro_hammingloss(pistar.predict(X_train), y_train))\n",
    "lstar = micro_hammingloss(pistar.predict(X_test), y_test)\n",
    "print(\"pi* test loss:\", lstar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd69647",
   "metadata": {},
   "source": [
    "---\n",
    "## Sequential CRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51001a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_init = np.random.normal(size=(X_train.shape[1], len(labels)))\n",
    "print('beta0 H. loss:', evaluate_model(beta_init, X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d035c476",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "beta_static = np.array(beta_init.copy())\n",
    "beta_dynamic = np.array(beta_init.copy())\n",
    "\n",
    "static_crm_dataset = ([],[],[],[])\n",
    "dynamic_crm_dataset = ([],[],[],[])\n",
    "\n",
    "n_episods = 10\n",
    "batch = int(len(X_train) / n_episods)\n",
    "\n",
    "t_end = t_start = time.time()\n",
    "for episod in range(n_episods):\n",
    "    t_end = time.time()\n",
    "    \n",
    "    start = episod*batch\n",
    "    end = (episod+1)*batch\n",
    "    print('*'*10, \n",
    "          'episod: %d/%d' % (episod+1, n_episods), \n",
    "          'time: %ds' % (t_end - t_start), \n",
    "          '*'*10,\n",
    "          file=sys.stderr)\n",
    "    \n",
    "    t_start = time.time()\n",
    "    \n",
    "    # current slice of dataset\n",
    "    X = X_train[start:end,:]\n",
    "    y = y_train[start:end,:]\n",
    "    \n",
    "    # generating CRM counter-part of current slice\n",
    "    sampling_probas_static = pi0.predict_proba(X)\n",
    "    sampling_probas_static = np.array([_[:,1] for _ in sampling_probas_static]).T\n",
    "    if episod == 0:\n",
    "        sampling_probas_dynamic = sampling_probas_static\n",
    "    else:\n",
    "        sampling_probas_dynamic = model_predict(beta_dynamic, X)\n",
    "\n",
    "    # optimizing models & evaluate\n",
    "    ## static CRM\n",
    "    print('** static policy', file=sys.stderr) \n",
    "    beta_static, static_crm_dataset, static_crm_loss = iterate_model(\n",
    "        beta_static, X, y, sampling_probas_static, static_crm_dataset\n",
    "    )\n",
    "    l_stat = evaluate_model(beta_static, X_test, y_test)\n",
    "    print('H. loss: %.5f (vs pi0: %d%% vs pi*: %d%%)' % (l_stat, 100*l_stat/l0, 100*l_stat/lstar), \n",
    "          '|beta|=%.4f' % np.sqrt((beta_static**2).sum()), \n",
    "          file=sys.stderr)\n",
    "    ## sequential CRM\n",
    "    print('** dynamic policy', file=sys.stderr) \n",
    "    beta_dynamic, dynamic_crm_dataset, dynamic_crm_loss = iterate_model(\n",
    "        beta_dynamic, X, y, sampling_probas_dynamic, dynamic_crm_dataset\n",
    "    )\n",
    "    l_dyn = evaluate_model(beta_dynamic, X_test, y_test)\n",
    "    print('H. loss: %.5f (vs pi0: %d%% vs pi*: %d%%)' % (l_dyn, 100*l_dyn/l0, 100*l_dyn/lstar), \n",
    "          '|beta|=%.4f' % np.sqrt((beta_dynamic**2).sum()), \n",
    "          file=sys.stderr)\n",
    "    print(file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf53deb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Sampling Probas per Action')\n",
    "plt.plot(sampling_probas_static.mean(axis=0),'--', label='static')\n",
    "plt.plot(sampling_probas_dynamic.mean(axis=0),'--', label='dynamic')\n",
    "plt.plot(y_test.mean(axis=0), label='label average', alpha=.5)\n",
    "plt.ylim(0,1)\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
